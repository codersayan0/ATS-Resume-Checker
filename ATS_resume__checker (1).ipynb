{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tMBbn1FynDMh"
      },
      "outputs": [],
      "source": [
        "!pip install numpy==1.26.4 scipy==1.10.1 --force-reinstall --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas==2.2.2 gensim==4.3.2 sentence-transformers==2.2.2 huggingface-hub==0.23.0"
      ],
      "metadata": {
        "id": "J2r7d0scw7QZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q spacy==3.7.2 nltk wordcloud seaborn scikit-learn matplotlib"
      ],
      "metadata": {
        "id": "VylYCj8jw7cS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q cqdm PyMuPDF pdfminer.six"
      ],
      "metadata": {
        "id": "2EThKYkWxC3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "id": "Y3yWfCLPxDBa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0 --index-url https://download.pytorch.org/whl/cu118"
      ],
      "metadata": {
        "id": "Q18YqLLmpdHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import spacy\n",
        "import nltk  # Correct spelling\n",
        "import fitz  # PyMuPDF works via the 'fitz' module\n",
        "\n",
        "import gensim.downloader as api\n",
        "from tqdm import tqdm\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import confusion_matrix, roc_curve, auc  # Corrected names\n"
      ],
      "metadata": {
        "id": "Rw7nL1xttUd7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from wordcloud import WordCloud\n",
        "from sentence_transformers import SentenceTransformer  # Fixed import path\n",
        "from collections import Counter\n",
        "\n",
        "# If using Google Colab\n",
        "from google.colab import files\n",
        "\n",
        "# Download NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Load SpaCy model\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")  # Corrected assignment and removed stray bracket\n"
      ],
      "metadata": {
        "id": "jTg5APfPvF7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import fitz  # PyMuPDF\n",
        "import os\n",
        "\n",
        "# Upload files\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Text extraction function\n",
        "def extract_text(file_path):\n",
        "    ext = os.path.splitext(file_path)[1].lower()\n",
        "\n",
        "    if ext == '.pdf':\n",
        "        doc = fitz.open(file_path)\n",
        "        return ''.join([page.get_text() for page in doc])\n",
        "    else:\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            return f.read()\n",
        "\n",
        "# Lists to store data\n",
        "resumes = []\n",
        "names = []\n",
        "\n",
        "# Process each uploaded file\n",
        "for file in uploaded:\n",
        "    text = extract_text(file)\n",
        "    resumes.append(text)\n",
        "    names.append(file)\n",
        "df = pd.DataFrame({'filename': names, 'raw_text': resumes})"
      ],
      "metadata": {
        "id": "LeI1dFyrv0_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(txt):\n",
        "    txt = txt.lower()\n",
        "    txt = re.sub(r'\\s+', ' ', txt)\n",
        "    txt = re.sub(r'[^\\w\\s]', '', txt)\n",
        "    return txt\n",
        "def extract_entities(text):\n",
        "    doc = nlp(text)\n",
        "    entities = {'name': None, 'email': None, 'phone': None}\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ == 'PERSON' and not entities['name']:\n",
        "            entities['name'] = ent.text\n",
        "    email = re.findall(r'\\S+@\\S+', text)\n",
        "    entities['email'] = email[0] if email else None\n",
        "    phone = re.findall(r'\\+?\\d[\\d-]{8,13}', text)\n",
        "    entities['phone'] = phone[0] if phone else None\n",
        "    return entities\n",
        "\n",
        "df['cleaned'] = df['raw_text'].apply(clean_text)\n",
        "entities = df['raw_text'].apply(extract_entities).apply(pd.Series)\n",
        "df = pd.concat([df, entities], axis=1)\n"
      ],
      "metadata": {
        "id": "zLkgG6wl1sg8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n"
      ],
      "metadata": {
        "id": "HytTRTs057XM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_skills = [\n",
        "    'python', 'java', 'sql', 'r', 'excel', 'communication', 'data analysis', 'nlp', 'ai',\n",
        "    'tensorflow', 'keras', 'pandas', 'numpy', 'flask', 'django', 'cloud', 'aws', 'linux'\n",
        "]\n",
        "w2v = api.load(\"glove-wiki-gigaword-100\")\n",
        "expanded_skills = set(base_skills)\n",
        "for skill in base_skills:\n",
        "    try:\n",
        "        similar = w2v.most_similar(skill, topn=3)\n",
        "        for word, score in similar:\n",
        "            expanded_skills.add(word)\n",
        "    except KeyError:\n",
        "        continue"
      ],
      "metadata": {
        "id": "J5eJmX126di7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_skills(text):\n",
        "    tokens = [t.lower() for t in word_tokenize(text) if t.isalpha()]\n",
        "    tokens = [t for t in tokens if t not in stop_words]\n",
        "    found = set(t for t in tokens if t in expanded_skills)\n",
        "    return list(found)\n",
        "df['skills'] = df['cleaned'].apply(extract_skills)"
      ],
      "metadata": {
        "id": "ypULQBmh7zoG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "jd_files_data = uploaded\n",
        "jd_texts = []\n",
        "jd_names = []\n",
        "for name, content in jd_files_data.items():\n",
        "    with open(name, 'wb') as f:\n",
        "        f.write(content)\n",
        "    text = extract_text(name)\n",
        "    jd_texts.append(clean_text(text))\n",
        "    jd_names.append(name)\n",
        "uploaded=()\n",
        "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
        "resume_embeddings = model.encode(df['cleaned'].tolist(), show_progress_bar=True)\n",
        "jd_embeddings = model.encode(jd_texts, show_progress_bar=True)\n",
        "similarity_matrix = cosine_similarity(resume_embeddings, jd_embeddings)\n",
        "best_match_idx = np.argmax(similarity_matrix, axis=1)\n",
        "best_scores = np.max(similarity_matrix, axis=1)\n",
        "df['best_match_jd'] = [jd_names[i] for i in best_match_idx]\n",
        "df['similarity_score'] = best_scores\n"
      ],
      "metadata": {
        "id": "vIJvVNKx-_3D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_n = 5\n",
        "for jd_name in jd_names:\n",
        "    print(f\"\\nTop {top_n} Matches for JD: {jd_name}\")\n",
        "\n",
        "    jd_matches = df[df['best_match_jd'] == jd_name] \\\n",
        "                   .sort_values(by='similarity_score', ascending=False) \\\n",
        "                   .head(top_n)\n",
        "\n",
        "    display(jd_matches[['filename', 'name', 'email', 'skills', 'similarity_score']])\n"
      ],
      "metadata": {
        "id": "mFJ_Thv5YgeY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "skill_freq = {}\n",
        "for skills in df['skills']:\n",
        "    for s in skills:\n",
        "        skill_freq[s] = skill_freq.get(s, 0) + 1\n",
        "wc = WordCloud(width=1000, height=500, background_color=\"white\")\n",
        "wc.generate_from_frequencies(skill_freq)\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.imshow(wc, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title(\"All Resume Skill Frequencies\")\n",
        "plt.show()\n",
        "sns.histplot(df['similarity_score'], kde=True, color=\"teal\")\n",
        "plt.title(\"Resume vs JD Similarity\", )\n",
        "plt.xlabel(\"Cosine Similarity\",)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fep-emI0ZLdx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "df['match'] = df['similarity_score'].apply(lambda x: 1 if x > 0.65 else 0)\n",
        "true = [1 if i < len(df)//2 else 0 for i in range(len(df))]\n",
        "pred = df['match'].tolist()\n",
        "precision = precision_score(true, pred)\n",
        "recall = recall_score(true, pred)\n",
        "f1 = f1_score(true, pred)\n",
        "classi = classification_report(true, pred)\n",
        "print(f\"Precision: {precision:.2f}, Recall: {recall:.2f}, F1 Score: {f1:.2f}\")\n",
        "print(classi)\n",
        "fpr, tpr, _ = roc_curve(true, df['similarity_score'])\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "plt.plot(fpr, tpr, label=f'AUC = {roc_auc:.2f}')\n",
        "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC Curve for Resume-JD Matching\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "N6hV9kWWgAnJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_clusters = 2\n",
        "if len(resume_embeddings) >= n_clusters:\n",
        "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10) # Add n_init\n",
        "    clusters = kmeans.fit_predict(resume_embeddings)\n",
        "    df['cluster'] = clusters\n",
        "    pca = PCA(n_components=2)\n",
        "    pca_components = pca.fit_transform(resume_embeddings)\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.scatterplot(x=pca_components[:, 0], y=pca_components[:, 1], hue=clusters, palette=\"Set2\")\n",
        "    plt.title(\"Resume Skill Clustering via T5 + PCA\")\n",
        "    plt.xlabel(\"PC1\")\n",
        "    plt.ylabel(\"PC2\")\n",
        "    plt.show()\n",
        "else:\n",
        "    print(f\"Cannot perform KMeans with {n_clusters} clusters. Need {n_clusters} resumes, but found {len(resume_embeddings)}.\")\n"
      ],
      "metadata": {
        "id": "Qo10Ng6AlWJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def jd_skill_words(text):\n",
        "    tokens = [t.lower() for t in word_tokenize(text) if t.isalpha()]\n",
        "    return [t for t in tokens if t not in stop_words and t in expanded_skills]\n",
        "\n",
        "for i, jd_text in enumerate(jd_texts):\n",
        "    jd_skills = jd_skill_words(jd_text)\n",
        "    jd_skill_freq = Counter(jd_skills)\n",
        "\n",
        "    resume_skills = []\n",
        "    for skills in df[df['best_match_jd'] == jd_names[i]]['skills']:\n",
        "        resume_skills.extend(skills)\n",
        "\n",
        "    resume_skill_freq = Counter(resume_skills)\n",
        "    common_skills = list(set(jd_skill_freq.keys()) & set(resume_skill_freq.keys()))\n",
        "    bar_vals = [resume_skill_freq[s] for s in common_skills]\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    sns.barplot(x=common_skills, y=bar_vals, palette=\"viridis\")\n",
        "    plt.title(f\"Skill Overlap Frequency (JD: {jd_names[i]})\")\n",
        "    plt.ylabel(\"Count in Matched Resumes\")\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "49ofxghSmVap"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}